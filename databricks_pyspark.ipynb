{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "3UpHznEJ2fGo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYqpgl0_geXj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "import re\n",
        "import datetime\n",
        "import dateparser\n",
        "import pytz\n",
        "\n",
        "from typing import List\n",
        "from tzlocal import get_localzone\n",
        "from pyspark.sql import functions as sqlf\n",
        "from pyspark.sql import types as sqlTypes\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql import DataFrame\n",
        "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
        "from functools import reduce\n",
        "from datetime import date, timedelta\n",
        "from dateutil.relativedelta import relativedelta\n",
        "\n",
        "\n",
        "from pyspark.sql.functions import when"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%sql\n",
        "\n",
        "# --fix legacy dates in Spark 2.0\n",
        "set spark.sql.legacy.parquet.datetimeRebaseModeInRead=CORRECTED;\n",
        "set spark.sql.shuffle.partitions=48;\n",
        "\n",
        "# --disable broadcast joins with larger data sets\n",
        "set spark.sql.autoBroadcastJoinThreshold=-1;\n",
        "\n",
        "# --set all properties for delta tables to optimize\n",
        "set spark.databricks.delta.properties.defaults.autoOptimize.optimizeWrite = true;\n",
        "set spark.databricks.delta.properties.defaults.autoOptimize.autoCompact = true;"
      ],
      "metadata": {
        "id": "7blPNW1n2iNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parameters"
      ],
      "metadata": {
        "id": "QpxBJN1r2__Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#truncate table param\n",
        "dbutils.widgets.text(\"EmptyTable\",\"FALSE\")\n",
        "EmptyTable = dbutils.widgets.get(\"EmptyTable\")\n",
        "\n",
        "#day to start load\n",
        "dbutils.widgets.text(\"sDate\",datetime.datetime.now().strftime('%Y-%m-%d'))\n",
        "sDate = dbutils.widgets.get(\"sDate\")\n",
        "\n",
        "if (sDate is None) | (sDate==''):\n",
        "  sDate = datetime.datetime.now()\n",
        "  sDate = sDate.strftime(\"%Y-%m-%d\")\n",
        "else:\n",
        "  sDate = datetime.datetime.strptime(sDate,'%Y-%m-%d')\n",
        "  \n",
        "#day to end load\n",
        "dbutils.widgets.text(\"eDate\", datetime.datetime.now().strftime('%Y-%m-%d') )\n",
        "eDate = dbutils.widgets.get(\"eDate\")\n",
        "\n",
        "if (eDate is None) | (eDate==''):\n",
        "  eDate = datetime.datetime.now()\n",
        "  eDate = eDate.strftime(\"%Y-%m-%d\")\n",
        "else:\n",
        "  eDate = datetime.datetime.strptime(eDate,'%Y-%m-%d')\n",
        "\n",
        "#set variable to be used in sql\n",
        "spark.conf.set('start.date', str(sDate))\n",
        "spark.conf.set('end.date', str(eDate))\n",
        "\n",
        "#get rolling 4 years\n",
        "rollingDate = (datetime.datetime.now(pytz.timezone('US/Central')) + relativedelta(months=-48)).strftime(\"%Y-%m-%d\")"
      ],
      "metadata": {
        "id": "thw6IqBh2qf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Database"
      ],
      "metadata": {
        "id": "vrp4Lxtu3ihq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# datalake path\n",
        "baseDataLakePath = \"/mnt/folder_path_name/\"\n",
        "\n",
        "# folder for read/write\n",
        "STATIC_FOLDER = baseDataLakePath + \"folder_name/\"\n",
        "TABLE_FOLDER = \"file_name/\"\n",
        "FINAL_FOLDER = str(STATIC_FOLDER) + str(TABLE_FOLDER)\n",
        "TMP_FOLDER = str(FINAL_FOLDER) + \"DELTA/\"\n",
        "\n",
        "# setup databricks database variables\n",
        "dbName = \"schema\"\n",
        "permTable = \"table_name\"\n",
        "\n",
        "#set variable to be used in sql\n",
        "spark.conf.set('delta.folder', str(TMP_FOLDER))\n",
        "spark.conf.set('db.table', str(dbName + \".\" + permTable))"
      ],
      "metadata": {
        "id": "kuO9hIl83lOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if EmptyTable.upper()==\"TRUE\":\n",
        "  spark.sql(\"DROP TABLE IF EXISTS \" + dbName + \".\" + permTable)\n",
        "  dbutils.fs.rm(FINAL_FOLDER + 'DELTA/',recurse=True);\n",
        "  dbutils.fs.rm(FINAL_FOLDER + 'LOAD/',recurse=True);"
      ],
      "metadata": {
        "id": "ui1se78K6LOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Data From Datalake and Convert to Delta Format"
      ],
      "metadata": {
        "id": "OBctxr0E6QCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# parquet file\n",
        "file_type = \"parquet\"\n",
        "\n",
        "from_file = SHARED_FOLDER + 'datalake_file_path/filename.parquet'\n",
        "to_file = FINAL_FOLDER + 'TMP_FILE/'\n",
        "\n",
        "if file_exists(to_file):\n",
        "  #delete file\n",
        "  dbutils.fs.rm(to_file, True)\n",
        "  \n",
        "df = spark.read.format(file_type) \\\n",
        "  .load(from_file)\n",
        "df.write.format(\"delta\").save(to_file)\n",
        "\n",
        "# csv file\n",
        "df = spark.read.format(\"csv\") \\\n",
        "  .option(\"header\", first_row_is_header) \\\n",
        "  .option(\"sep\", delimiter) \\\n",
        "  .load(FINAL_FOLDER + 'filename.csv')\n",
        "\n",
        "\n",
        "to_file = FINAL_FOLDER + 'TMP_FILE/'\n",
        "\n",
        "if file_exists(to_file):\n",
        "  #delete file\n",
        "  dbutils.fs.rm(to_file, True)\n",
        "#write csv data to delta  \n",
        "df.write.format(\"delta\").save(to_file)\n",
        "\n",
        "#clean up\n",
        "df.unpersist()\n",
        "spark.sparkContext._jvm.System.gc()"
      ],
      "metadata": {
        "id": "Vqzk6qHr6LqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read data back in as views"
      ],
      "metadata": {
        "id": "sHEd98bS7cCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_location = FINAL_FOLDER + 'TMP_FILE/'\n",
        "file_type = \"delta\"\n",
        "\n",
        "df = spark.read.format(file_type).load(file_location)\n",
        "#create temp view\n",
        "df.registerTempTable(\"vw_table_name\")"
      ],
      "metadata": {
        "id": "wK11XFYI7frC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformations"
      ],
      "metadata": {
        "id": "ZgyVX7M770nM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%sql\n",
        "\n",
        "DROP VIEW IF EXISTS vw_final;\n",
        "CREATE TEMPORARY VIEW vw_final\n",
        "As\n",
        "Select Distinct *\n",
        "From vw_table_name"
      ],
      "metadata": {
        "id": "eY3RLb4W736O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load up data\n",
        "theSQL = \"Select distinct * From vw_final Where Date>='\" + str(rollingDate) + \"'\"\n",
        "tmp_final_df = spark.sql(theSQL)\n",
        "\n",
        "# dropping one record\n",
        "tmp_final_df = tmp_final_df.withColumn('New_col',\n",
        "\t\t\t\t\twhen(tmp_final_df.column1 != 'value1', \"True\")\n",
        "\t\t\t\t\t.when(tmp_final_df.column2 != 'value2', \"True\")\n",
        "                    .when(tmp_final_df.column3 != 'value3', \"True\")\n",
        "                    .when(tmp_final_df.column4 != 'value4', \"True\")\n",
        "\t\t\t\t\t).filter(\"New_col == True\").drop(\"New_col\")"
      ],
      "metadata": {
        "id": "JIgGUxbp80F-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UDF Functions"
      ],
      "metadata": {
        "id": "pHrXRr8g-ASs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sql = 'select * from tmp_final_df'\n",
        "final_df = sqlContext.sql(sql)\n",
        "\n",
        "def myFunction(column_value):\n",
        "  return None\n"
      ],
      "metadata": {
        "id": "Zc47Jm1p-HbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply UDF\n",
        "get_udf = udf(lambda i: df_column_name(i))\n",
        "final_df = final_df.withColumn('New_Column_Output_From_UDF', get_udf('df_column_name'))"
      ],
      "metadata": {
        "id": "gap4n3Wa-HQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial Data Load to Delta Folder - Only do on first load of data"
      ],
      "metadata": {
        "id": "Xl1NxCaU_BVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#sql = \"Select * From tmp_site\"\n",
        "#final_df = spark.sql(sql)\n",
        "\n",
        "#set up first load variables\n",
        "isFirstLoad = False\n",
        "\n",
        "first_file_location = FINAL_FOLDER + 'DELTA/'\n",
        "\n",
        "if not file_exists(first_file_location):\n",
        "  #drop table\n",
        "  spark.sql(\"DROP TABLE IF EXISTS \" + str(dbName + \".\" + permTable))\n",
        "  final_df.write.format(\"delta\").save(first_file_location)\n",
        "  spark.sql(\"ALTER TABLE delta.`\" + FINAL_FOLDER + \"DELTA/` SET TBLPROPERTIES (delta.autoOptimize.optimizeWrite = true)\")\n",
        "  isFirstLoad = True"
      ],
      "metadata": {
        "id": "1Z0C1Dcl_Cdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial Data Load Table Creation - Only do on first load of data"
      ],
      "metadata": {
        "id": "RLWd1Arl_WiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%sql\n",
        "CREATE DATABASE IF NOT EXISTS schema_name;\n",
        "\n",
        "# --create the table where the data will go from here foward\n",
        "CREATE TABLE IF NOT EXISTS ${db.table}\n",
        "USING DELTA \n",
        "LOCATION '${delta.folder}';\n",
        "\n",
        "# --clean up\n",
        "DROP VIEW IF EXISTS vw_final;"
      ],
      "metadata": {
        "id": "C73lm0P8_Xoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Deltas and write to databricks db and load folder"
      ],
      "metadata": {
        "id": "hWWfFoEE_x4E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#only do if not first load\n",
        "if not isFirstLoad:\n",
        "  \n",
        "  #optimize table\n",
        "  optSql = \"OPTIMIZE \" + dbName + \".\" + permTable + \" ZORDER BY (\" + ','.join(fieldJoins) + \")\"\n",
        "  spark.sql(optSql)\n",
        "  \n",
        "  #get deltas to load\n",
        "  \n",
        "  #get latest file uploaded\n",
        "  historySQL = \"Select * From \" + dbName + \".\" + permTable\n",
        "  df_HISTORY = spark.sql(historySQL)\n",
        "  \n",
        "  #match the schema and fields in the existing table\n",
        "  final_df = match_Schema(df_HISTORY,final_df)\n",
        "\n",
        "  #drop UTC date field\n",
        "  df_HISTORY = df_HISTORY.drop('EXTRACTUTCDATE')\n",
        "  final_df = final_df.drop('EXTRACTUTCDATE')\n",
        "\n",
        "  #find records that are not in or different from history to load (deltas)\n",
        "  df_deltas = final_df.exceptAll(df_HISTORY)\n",
        "  df_deltas.drop_duplicates()\n",
        "\n",
        "  #add back in utc date\n",
        "  df_deltas = df_deltas.withColumn(\"EXTRACTUTCDATE\", sqlf.lit(datetime.datetime.now()))\n",
        "\n",
        "  #write file\n",
        "  file_location = FINAL_FOLDER + 'LOAD/'\n",
        "  df_deltas.write.mode(\"Overwrite\").format(\"parquet\").save(file_location)\n",
        "\n",
        "  #create temp table to hold updates\n",
        "  df_deltas.registerTempTable(\"latestRecords\")\n",
        "\n",
        "  #create dynamic join string\n",
        "  joinStg = \"\"\n",
        "  for x in range(0, len(fieldJoins)):\n",
        "    if x==0:\n",
        "      joinStg += \" (\" + permTable + \".\" + str(fieldJoins[x]) + \"=\" + tmpTable + \".\" + str(fieldJoins[x]) + \")\"\n",
        "    else:\n",
        "      joinStg += \" And (\" + permTable + \".\" + str(fieldJoins[x]) + \"=\" + tmpTable + \".\" + str(fieldJoins[x]) + \")\"\n",
        "\n",
        "  #create overall sql to run merge\n",
        "  mainSQL = \"MERGE INTO \" + dbName + \".\" + permTable + \" USING \" + tmpTable + \" ON (\" + joinStg + \") \" + \\\n",
        "            \"WHEN MATCHED THEN UPDATE SET * \" + \\\n",
        "            \"WHEN NOT MATCHED THEN INSERT *\"\n",
        "\n",
        "  spark.sql(mainSQL)\n",
        "\n",
        "  #remove data frame\n",
        "  final_df.unpersist()\n",
        "  df_deltas.unpersist()\n",
        "\n",
        "else:\n",
        "  \n",
        "  #write file\n",
        "  file_location = FINAL_FOLDER + 'LOAD/'\n",
        "  final_df.write.mode(\"Overwrite\").format(\"parquet\").save(file_location)\n",
        "\n",
        "#clean up memory before closing app\n",
        "spark.sparkContext._jvm.System.gc()\n"
      ],
      "metadata": {
        "id": "nXY55rjbAgHp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}